apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-api
  namespace: llm-demo
  labels:
    app: llm-api
    version: v1.0
spec:
  replicas: 2  # Run 2 pods for high availability
  selector:
    matchLabels:
      app: llm-api
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # Create 1 extra pod during updates
      maxUnavailable: 0  # Keep all pods running during updates (zero downtime)
  template:
    metadata:
      labels:
        app: llm-api
        version: v1.0
    spec:
      # Security: Run as non-root user
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      
      containers:
      - name: llm-api
        image: bosofisan/llm-inference-platform:v1.0
        imagePullPolicy: IfNotPresent
        
        ports:
        - name: http
          containerPort: 8000
          protocol: TCP
        
        # Environment variables from ConfigMap
        envFrom:
        - configMapRef:
            name: llm-api-config
        
        # Sensitive env vars from Secret
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: llm-api-secrets
              key: openai-api-key
        
        # Resource limits and requests
        resources:
          requests:
            cpu: 100m    
            memory: 128Mi  
          limits:
            cpu: 500m      
            memory: 512Mi  
        
        # Liveness probe 
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
        
        # Readiness probe
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
        
        # Graceful shutdown
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 5"]
      
      # Spread pods across nodes for HA
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - llm-api
              topologyKey: kubernetes.io/hostname
